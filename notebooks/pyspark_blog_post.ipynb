{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%session_id_prefix ${account-name}-${iam-user-name}\n",
    "%iam_role arn:aws:iam::${AWS::AccountId}:role/${Glue-service-role-name}\n",
    "%number_of_workers 1\n",
    "%worker_type Standard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cell 1\n",
    "import sys\n",
    "from pyspark.context import SparkContext\n",
    "from awsglue.context import GlueContext\n",
    "from awsglue.transforms import *\n",
    "from awsglue.dynamicframe import DynamicFrame\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import Row\n",
    "glueContext = GlueContext(SparkContext.getOrCreate())\n",
    "\n",
    "print(\"===\")\n",
    "print(\"Orders Test\")\n",
    "print(\"===\")\n",
    "\n",
    "# cell 2\n",
    "order_list = [\n",
    "               ['1005', '623', 'YES', '1418901234', '75091'],\\\n",
    "               ['1006', '547', 'NO', '1418901256', '75034'],\\\n",
    "               ['1007', '823', 'YES', '1418901300', '75023'],\\\n",
    "               ['1008', '912', 'NO', '1418901400', '82091'],\\\n",
    "               ['1009', '321', 'YES', '1418902000', '90093']\\\n",
    "             ]\n",
    "\n",
    "# Define schema for the order_list\n",
    "order_schema = StructType([  \n",
    "                      StructField(\"order_id\", StringType()),\n",
    "                      StructField(\"customer_id\", StringType()),\n",
    "                      StructField(\"essential_item\", StringType()),\n",
    "                      StructField(\"timestamp\", StringType()),\n",
    "                      StructField(\"zipcode\", StringType())\n",
    "                    ])\n",
    "\n",
    "# Create a Spark Dataframe from the python list and the schema\n",
    "df_orders = spark.createDataFrame(order_list, schema = order_schema)\n",
    "df_orders.show()\n",
    "\n",
    "# cell 3\n",
    "dyf_orders = DynamicFrame.fromDF(df_orders, glueContext, \"dyf\")\n",
    "\n",
    "# Input \n",
    "dyf_applyMapping = ApplyMapping.apply( frame = dyf_orders, mappings = [ \n",
    "  (\"order_id\",\"String\",\"order_id\",\"Long\"), \n",
    "  (\"customer_id\",\"String\",\"customer_id\",\"Long\"),\n",
    "  (\"essential_item\",\"String\",\"essential_item\",\"String\"),\n",
    "  (\"timestamp\",\"String\",\"timestamp\",\"Long\"),\n",
    "  (\"zipcode\",\"String\",\"zip\",\"Long\")\n",
    "])\n",
    "dyf_applyMapping.printSchema()\n",
    "\n",
    "# cell 4\n",
    "# Input \n",
    "dyf_filter = Filter.apply(frame = dyf_applyMapping, f = lambda x: x[\"essential_item\"] == 'YES')\n",
    "dyf_filter.toDF().show()\n",
    "\n",
    "# cell 5\n",
    "# Input\n",
    "\n",
    "# This function takes in a dynamic frame record and checks if zipcode\n",
    "# 75034 is present in it. If present, it adds another column \n",
    "# “next_day_air” with value as True\n",
    "def next_day_air(rec):\n",
    "  if rec[\"zip\"] == 75034:\n",
    "    rec[\"next_day_air\"] = True\n",
    "  return rec\n",
    "\n",
    "mapped_dyF =  Map.apply(frame = dyf_applyMapping, f = next_day_air)\n",
    "mapped_dyF.toDF().show()\n",
    "\n",
    "print(\"===\")\n",
    "print(\"Customers Test\")\n",
    "print(\"===\")\n",
    "\n",
    "# cell 6\n",
    "# Input \n",
    "jsonStr1 = u'{ \"zip\": 75091, \"customers\": [{ \"id\": 623, \"address\": \"108 Park Street, TX\"}, { \"id\": 231, \"address\": \"763 Marsh Ln, TX\" }]}'\n",
    "jsonStr2 = u'{ \"zip\": 82091, \"customers\": [{ \"id\": 201, \"address\": \"771 Peek Pkwy, GA\" }]}'\n",
    "jsonStr3 = u'{ \"zip\": 75023, \"customers\": [{ \"id\": 343, \"address\": \"66 P Street, NY\" }]}'\n",
    "jsonStr4 = u'{ \"zip\": 90093, \"customers\": [{ \"id\": 932, \"address\": \"708 Fed Ln, CA\"}, { \"id\": 102, \"address\": \"807 Deccan Dr, CA\" }]}'\n",
    "df_row = spark.createDataFrame([\n",
    "  Row(json=jsonStr1),\n",
    "  Row(json=jsonStr2),\n",
    "  Row(json=jsonStr3),\n",
    "  Row(json=jsonStr4)\n",
    "])\n",
    "\n",
    "df_json = spark.read.json(df_row.rdd.map(lambda r: r.json))\n",
    "df_json.show()\n",
    "\n",
    "# cell 7\n",
    "# Input\n",
    "df_json.printSchema()\n",
    "\n",
    "# cell 8\n",
    "# Input\n",
    "dyf_json = DynamicFrame.fromDF(df_json, glueContext, \"dyf_json\")\n",
    "dyf_json.toDF().show()\n",
    "\n",
    "# cell 9\n",
    "# Input\n",
    "dyf_selectFields = SelectFields.apply(frame = dyf_filter, paths=['zip'])\n",
    "dyf_selectFields.toDF().show()\n",
    "\n",
    "# cell 10\n",
    "# Input\n",
    "dyf_join = Join.apply(dyf_json, dyf_selectFields, 'zip', 'zip')\n",
    "dyf_join.toDF().show()\n",
    "\n",
    "# cell 11\n",
    "# Input\n",
    "dyf_dropfields = DropFields.apply(\n",
    "  frame = dyf_join,\n",
    "  paths = \"`.zip`\"\n",
    ")\n",
    "dyf_dropfields.toDF().show()\n",
    "\n",
    "# cell 12\n",
    "# Input\n",
    "dyf_relationize = dyf_dropfields.relationalize(\"root\", \"s3://glue-interactive-sessions-demo-${AWS::AccountId}/GlueLocalOutput/\")\n",
    "dyf_relationize.keys()\n",
    "\n",
    "# cell 13\n",
    "# Input\n",
    "dyf_selectFromCollection = SelectFromCollection.apply(dyf_relationize, 'root')\n",
    "dyf_selectFromCollection.toDF().show()\n",
    "\n",
    "# cell 14\n",
    "# Input\n",
    "dyf_selectFromCollection = SelectFromCollection.apply(dyf_relationize, 'root_customers')\n",
    "dyf_selectFromCollection.toDF().show()\n",
    "\n",
    "# cell 15\n",
    "# Input\n",
    "dyf_renameField_1 = RenameField.apply(dyf_selectFromCollection, \"`customers.val.address`\", \"address\")\n",
    "dyf_renameField_2 = RenameField.apply(dyf_renameField_1, \"`customers.val.id`\", \"cust_id\")\n",
    "dyf_dropfields_rf = DropFields.apply(\n",
    "  frame = dyf_renameField_2,\n",
    "  paths = [\"index\", \"id\"]\n",
    ")\n",
    "dyf_dropfields_rf.toDF().show()\n",
    "\n",
    "# cell 16\n",
    "# Input\n",
    "dyf_resolveChoice = dyf_dropfields_rf.resolveChoice(specs = [('cust_id','cast:String')])\n",
    "dyf_resolveChoice.printSchema()\n",
    "\n",
    "print(\"===\")\n",
    "print(\"Warehouse Test\")\n",
    "print(\"===\")\n",
    "\n",
    "# cell 17\n",
    "# Input\n",
    "warehouse_inventory_list = [\n",
    "              ['TX_WAREHOUSE', '{\\\n",
    "                          \"strawberry\":\"220\",\\\n",
    "                          \"pineapple\":\"560\",\\\n",
    "                          \"mango\":\"350\",\\\n",
    "                          \"pears\":null}'\n",
    "               ],\\\n",
    "              ['CA_WAREHOUSE', '{\\\n",
    "                         \"strawberry\":\"34\",\\\n",
    "                         \"pineapple\":\"123\",\\\n",
    "                         \"mango\":\"42\",\\\n",
    "                         \"pears\":null}\\\n",
    "              '],\n",
    "    \t\t   ['CO_WAREHOUSE', '{\\\n",
    "                         \"strawberry\":\"340\",\\\n",
    "                         \"pineapple\":\"180\",\\\n",
    "                         \"mango\":\"2\",\\\n",
    "                         \"pears\":null}'\n",
    "              ]\n",
    "            ]\n",
    "\n",
    "\n",
    "warehouse_schema = StructType([StructField(\"warehouse_loc\", StringType())\\\n",
    "                              ,StructField(\"data\", StringType())])\n",
    "\n",
    "df_warehouse = spark.createDataFrame(warehouse_inventory_list, schema = warehouse_schema)\n",
    "dyf_warehouse = DynamicFrame.fromDF(df_warehouse, glueContext, \"dyf_warehouse\")\n",
    "\n",
    "dyf_warehouse.printSchema()\n",
    "\n",
    "# cell 18\n",
    "# Input\n",
    "dyf_unbox = Unbox.apply(frame = dyf_warehouse, path = \"data\", format=\"json\")\n",
    "dyf_unbox.printSchema()\n",
    "\n",
    "# cell 19\n",
    "dyf_unbox.toDF().show()\n",
    "\n",
    "# cell 20\n",
    "# Input\n",
    "dyf_unnest = UnnestFrame.apply(frame = dyf_unbox)\n",
    "dyf_unnest.printSchema()\n",
    "\n",
    "# cell 21\n",
    "dyf_unnest.toDF().show()\n",
    "\n",
    "# cell 22\n",
    "# Input\n",
    "dyf_dropNullfields = DropNullFields.apply(frame = dyf_unnest)\n",
    "dyf_dropNullfields.toDF().show()\n",
    "\n",
    "# cell 23\n",
    "# Input\n",
    "dyf_splitFields = SplitFields.apply(frame = dyf_dropNullfields, paths = [\"`data.strawberry`\", \"`data.pineapple`\"], name1 = \"a\", name2 = \"b\")\n",
    "\n",
    "# cell 24\n",
    "# Input\n",
    "dyf_retrieve_a = SelectFromCollection.apply(dyf_splitFields, \"a\")\n",
    "dyf_retrieve_a.toDF().show()\n",
    "\n",
    "# cell 25\n",
    "# Input\n",
    "dyf_retrieve_b = SelectFromCollection.apply(dyf_splitFields, \"b\")\n",
    "dyf_retrieve_b.toDF().show()\n",
    "\n",
    "# cell 26\n",
    "# Input\n",
    "dyf_splitRows = SplitRows.apply(frame = dyf_dropNullfields, comparison_dict = {\"`data.pineapple`\": {\">\": \"100\", \"<\": \"200\"}}, name1 = 'pa_200_less', name2 = 'pa_200_more')\n",
    "\n",
    "# cell 27\n",
    "# Input\n",
    "dyf_pa_200_less = SelectFromCollection.apply(dyf_splitRows, 'pa_200_less')\n",
    "dyf_pa_200_less.toDF().show()\n",
    "\n",
    "# cell 28\n",
    "# Input\n",
    "dyf_pa_200_more = SelectFromCollection.apply(dyf_splitRows, 'pa_200_more')\n",
    "dyf_pa_200_more.toDF().show()\n",
    "\n",
    "# cell 29\n",
    "# Input\n",
    "dyf_spigot = Spigot.apply(\\\n",
    "frame = dyf_pa_200_less,\\\n",
    "path = \"s3://glue-interactive-sessions-demo-${AWS::AccountId}/GlueLocalOutput/Spigot\",\\\n",
    "options = {\"topk\":10})\n",
    "dyf_spigot.toDF().show()\n",
    "\n",
    "# cell 30\n",
    "# Input\n",
    "glueContext.write_dynamic_frame.from_options(\\\n",
    "frame = dyf_spigot,\\\n",
    "connection_type = 's3',\\\n",
    "connection_options = {\"path\": \"s3://glue-interactive-sessions-demo-${AWS::AccountId}/GlueLocalOutput/\"},\\\n",
    "format = 'json')\n",
    "\n",
    "print(\"===\")\n",
    "print(\"The End\")\n",
    "print(\"===\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%list_sessions\n",
    "%stop_session\n",
    "%delete_session"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8a3ea62c2968271d117ef963590dc34aba8aeb70269f75d9781a51c13ac76054"
  },
  "kernelspec": {
   "display_name": "Glue PySpark",
   "language": "python",
   "name": "glue_python_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "Python_Glue_Session",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
